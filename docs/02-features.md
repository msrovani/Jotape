# üõ† Funcionalidades Detalhadas (com Foco em Supabase e Backend IA)

Esta se√ß√£o descreve o fluxo de dados e as intera√ß√µes entre o app Android, o Backend de IA e o Supabase para as funcionalidades principais.

## 3.0 Autentica√ß√£o (Login, Cadastro, Logout)

O fluxo de autentica√ß√£o permite que os usu√°rios acessem o aplicativo de forma segura.

1.  **Interface (App Android):**
    *   `LoginScreen`: Permite ao usu√°rio entrar com email/senha ou usando provedores OAuth (Google Sign-In).
    *   `SignUpScreen`: Permite ao usu√°rio criar uma nova conta com email/senha.
    *   Feedback de erro/sucesso √© exibido nessas telas.
    *   Navega√ß√£o entre Login/Cadastro e para a tela principal (`ConversationScreen`) ap√≥s sucesso.

2.  **Gerenciamento de Estado (App Android - Camada de Apresenta√ß√£o):
    *   `AuthViewModel` (Idealmente): Orquestra o fluxo de autentica√ß√£o, mant√©m o estado da UI (isLoading, errorMessage, isUserLoggedIn, isSignUpSuccessful) e chama o reposit√≥rio.
    *   *Nota:* Atualmente, essa l√≥gica est√° temporariamente no `ConversationViewModel`, mas deve ser refatorada para um `AuthViewModel` dedicado para seguir a Clean Architecture.

3.  **L√≥gica de Dados (App Android - Camada de Dados):
    *   `AuthRepository`: Abstrai a fonte de dados de autentica√ß√£o. Implementa fun√ß√µes como `signInWithEmail`, `signUpWithEmail`, `signInWithGoogle`, `signOut`, `getCurrentUserSession`.

4.  **Intera√ß√£o com Backend (App Android - Camada de Dados):
    *   `Supabase Kotlin Client`:
        *   `Auth`: Usado diretamente pelo `AuthRepository` para chamadas de `signInWith(Email)`, `signUpWith(Email)`, `signOut()`, `sessionStatus` (para observar mudan√ßas).
        *   `ComposeAuth`: Usado pelo `AuthViewModel`/`AuthRepository` (ou UI via `rememberSignInLauncher`) para simplificar o fluxo de `signInWith(Google)`.

5.  **Processo de Cadastro (Supabase):
    *   Ao chamar `signUpWithEmail`, o Supabase cria o registro do usu√°rio.
    *   **Confirma√ß√£o de Email:** Por padr√£o (e recomendado), o Supabase envia um email de confirma√ß√£o. O usu√°rio s√≥ consegue fazer login *ap√≥s* clicar no link de confirma√ß√£o. A UI deve informar o usu√°rio sobre isso ap√≥s o cadastro.

6.  **Seguran√ßa (Supabase):
    *   **JWT:** Ap√≥s login bem-sucedido, o Supabase Client armazena um JSON Web Token (JWT) que √© enviado em todas as requisi√ß√µes subsequentes.
    *   **RLS:** As Row-Level Security policies nas tabelas do Supabase DB (`interactions`, `user_profiles`, etc.) usam `auth.uid()` (extra√≠do do JWT) para garantir que os usu√°rios s√≥ acessem seus pr√≥prios dados (ver `05-security-privacy.md`).

## 3.1 Reconhecimento de Voz do Dono (Verifica√ß√£o)

O objetivo √© verificar se a voz do usu√°rio corresponde a um modelo previamente treinado e armazenado de forma segura, **utilizando o Backend de IA para o processamento pesado**.

1.  **`RecordVoiceSample()` (App Android):**
    *   Grava uma amostra de √°udio usando `MediaRecorder` ou similar.
    *   *Seguran√ßa:* Criptografar localmente antes do upload.
    *   Faz upload do arquivo de √°udio para o **Supabase Storage** (bucket seguro `private/voice_samples/{user_id}/`).

2.  **`TriggerVoiceProcessing()` (App Android):**
    *   Chama uma **Supabase Edge Function** (e.g., `trigger-voice-processing`) via Retrofit/OkHttp, atuando como um *gateway seguro*.
    *   Passa a refer√™ncia (path) do arquivo de √°udio no Storage e a inten√ß√£o (`train` ou `verify`).

3.  **Edge Function `trigger-voice-processing` (Supabase - TypeScript/Deno):**
    *   **Autentica√ß√£o:** Valida o JWT do usu√°rio para obter o `user_id` seguro.
    *   **Invoca√ß√£o do Backend IA:** Chama um endpoint espec√≠fico no **Backend de IA** (e.g., `/process-voice`), passando o `user_id`, `audioStoragePath` e `intent`.
    *   **Relay da Resposta:** Aguarda a resposta do Backend IA e a retorna para o App Android.

4.  **Processamento no Backend de IA (Python):**
    *   **Recebe Requisi√ß√£o:** Obt√©m `user_id`, `audioStoragePath`, `intent` da Edge Function.
    *   **Busca √Åudio:** Usa credenciais apropriadas (ou a identidade passada pela Edge Function) para baixar o √°udio do **Supabase Storage**.
    *   **Busca Modelo Existente (se `verify` ou retreinando):** Consulta a tabela `voice_models` no **Supabase DB** (via API ou conex√£o direta) para obter o `model_storage_path` do modelo atual do usu√°rio. Baixa o arquivo do modelo do **Supabase Storage**.
    *   **Processamento de Voz:** Usa bibliotecas Python (e.g., para extra√ß√£o de MFCCs, treinamento de GMM, ou outro m√©todo de Speaker Verification) para:
        *   **Treinar:** Gerar um novo modelo a partir da amostra de √°udio. Salva o arquivo do modelo no **Supabase Storage** (e.g., `private/voice_models/{user_id}/model_vX.bin`). Atualiza/Insere metadados (incluindo o novo `model_storage_path`) na tabela `voice_models` no **Supabase DB**.
        *   **Verificar:** Comparar os MFCCs da amostra atual com o modelo existente carregado.
    *   **Limpeza:** Pode deletar o arquivo de *√°udio bruto* do Storage ap√≥s o processamento.
    *   **Retorna Resultado:** Envia JSON com o resultado (`success`, `message`, `isVerified` ou metadados do modelo treinado) de volta para a Edge Function.

5.  **`HandleVerificationResult()` (App Android):**
    *   Recebe a resposta retransmitida pela Edge Function.
    *   Atualiza o estado da UI (e.g., mostra mensagem de sucesso/falha).
    *   Usa **TextToSpeech (TTS) nativo do Android** para feedback aud√≠vel *secund√°rio* (pois a voz principal vir√° do Backend IA).

## 3.2 Reconhecimento de Fala (Speech-to-Text - STT)

Converte a fala do usu√°rio em texto usando o **Backend de IA**.

1.  **`StartListening()` (App Android):**
    *   Utiliza `MediaRecorder` ou APIs de √°udio de baixo n√≠vel para capturar √°udio.
    *   *Opcional:* Implementar detec√ß√£o de atividade de voz (VAD) local (e.g., `webrtcvad`) e chunking (enviar peda√ßos de √°udio) para streaming e menor lat√™ncia, como sugerido em `Prompt-AI.txt`.
    *   Gerencia permiss√µes de √°udio e atualiza a UI (`isListening`).

2.  **`SendAudioToBackend()` (App Android):**
    *   Envia os chunks de √°udio (ou o arquivo completo) para um endpoint espec√≠fico da **API do Backend de IA** (e.g., `/stt` via WebSocket para streaming ou POST para arquivos completos).
    *   Inclui metadados necess√°rios (formato do √°udio, idioma, `user_id`).

3.  **Processamento STT no Backend de IA (Python):**
    *   Recebe o √°udio do App Android.
    *   Utiliza um modelo ASR (e.g., **Whisper.cpp**, **Vosk**) para transcrever o √°udio em texto.
    *   Pode realizar p√≥s-processamento (pontua√ß√£o, formata√ß√£o).
    *   Retorna a transcri√ß√£o (JSON com `{text, timestamps, confidence}`, como em `Prompt-AI.txt`) para o App Android.

4.  **`HandleTranscriptionResult()` (App Android):**
    *   Recebe o texto transcrito do Backend IA.
    *   Atualiza a UI para exibir o texto reconhecido.
    *   Prepara para enviar o texto para o processamento de IA (pr√≥xima se√ß√£o).

## 3.3 Processamento de IA e Gera√ß√£o de Resposta (RAG + LLM)

Envia o texto do usu√°rio para o **Backend de IA** para obter uma resposta contextualizada e inteligente.

1.  **`SendTextToBackend()` (App Android):**
    *   Envia o texto transcrito (obtido do STT na etapa anterior) para um endpoint da **API do Backend de IA** (e.g., `/generate-response`).
    *   Inclui `user_id` e, opcionalmente, hist√≥rico recente da conversa (para contexto) ou outros metadados relevantes.

2.  **Pipeline RAG e LLM no Backend de IA (Python):**
    *   **Recebe Requisi√ß√£o:** Obt√©m texto do usu√°rio, `user_id`, hist√≥rico.
    *   **1. Embedding da Entrada:** Gera embedding do texto do usu√°rio (e.g., usando **SentenceTransformers**).
    *   **2. Busca Vetorial (Mem√≥ria):** Consulta o √≠ndice vetorial (e.g., usando **`pgvector`** no **Supabase DB**) para encontrar intera√ß√µes passadas semanticamente similares (`candidates`) do `user_id`.
    *   **3. Constru√ß√£o do Prompt:** Cria um prompt otimizado para o LLM, incluindo:
        *   Instru√ß√µes de sistema (persona, tarefa).
        *   Hist√≥rico da conversa atual.
        *   Informa√ß√µes recuperadas da busca vetorial (`candidates`).
        *   Potencialmente, informa√ß√µes do perfil do usu√°rio ou `entitlements` (tier Free/Premium).
    *   **4. Chamada ao LLM:** Envia o prompt para o modelo de linguagem configurado (e.g., API do Google Gemini, OpenAI, etc.). Pode usar streaming se suportado.
    *   **5. Controle √âtico:** Valida a resposta bruta do LLM usando um modelo de modera√ß√£o ou regras internas (`EthicalControlService`) para filtrar conte√∫do inadequado.
    *   **6. Personaliza√ß√£o:** Ajusta a resposta final (estilo, formata√ß√£o) com base nas configura√ß√µes/persona do usu√°rio.
    *   **Retorna Resposta:** Envia a resposta textual final para o App Android.

3.  **`HandleBackendResponse()` (App Android):**
    *   Recebe a resposta textual do Backend IA.
    *   Exibe a resposta na `TranscriptView`.
    *   Inicia a s√≠ntese de voz (pr√≥xima se√ß√£o).

## 3.4 S√≠ntese de Voz (Text-to-Speech - TTS)

Converte a resposta textual do assistente em √°udio usando o **Backend de IA**.

1.  **`RequestTTSFromBackend()` (App Android):**
    *   Envia o texto da resposta final (recebido na etapa anterior) para um endpoint da **API do Backend de IA** (e.g., `/tts`).
    *   Inclui `user_id` e prefer√™ncias de voz (se aplic√°vel, baseado em `entitlements` ou IAPs).

2.  **Gera√ß√£o de TTS no Backend de IA (Python):**
    *   Recebe o texto e as prefer√™ncias de voz.
    *   Consulta o `EntitlementManager` para determinar a voz/modelo TTS apropriado (e.g., voz padr√£o para Free, vozes premium/customizadas para Premium/IAP).
    *   Usa um mecanismo TTS (e.g., **Coqui TTS**, **Mozilla TTS**) para converter o texto em √°udio.
    *   Aplica formata√ß√£o SSML, se necess√°rio, para melhor pros√≥dia.
    *   Envia o √°udio gerado (e.g., stream WAV/OGG) de volta para o App Android (via WebSocket ou HTTP streaming).

3.  **`PlaySynthesizedAudio()` (App Android):**
    *   Recebe o stream de √°udio do Backend IA.
    *   Utiliza `MediaPlayer` ou `ExoPlayer` para reproduzir o √°udio recebido.
    *   Atualiza a UI para indicar que o assistente est√° "falando".

---
*Nota: As se√ß√µes sobre UI (03) e Feedback/M√©tricas (04) podem precisar de pequenos ajustes para refletir que STT/TTS principais v√™m do backend, mas a estrutura geral permanece v√°lida.*